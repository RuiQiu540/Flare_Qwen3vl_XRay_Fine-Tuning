# Main SFT config for FLARE Task 5 (Qwen3-VL-4B-Instruct)

data:
  train_jsonl: data/train_sft.jsonl
  val_jsonl: data/val_sft.jsonl
  image_root: /path/to/FLARE_Task5

model:
  name: unsloth/Qwen3-VL-4B-Instruct
  max_seq_length: 12000

training:
  output_dir: checkpoints/SFT_full
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  warmup_ratio: 0.03
  weight_decay: 0.0
  max_steps: 1500
  early_stopping_patience: 5

logging:
  logging_steps: 10
  save_steps: 500
  eval_steps: 100
  report_to: wandb
  wandb_project: flare_task5_sft
  wandb_run_name: qwen3vl_sft_full

environment:
  platform: "Alliance (Compute Canada) Fir cluster"
  slurm:
    comment: "SFT jobs submitted with sbatch"
    gpus: "--gpus=h100:1"
    cpus_per_task: 8
    mem: "80G"
  hardware: >
    Experiments were run on Fir GPU nodes with 1× NVIDIA H100 80GB GPU,
    8 CPU cores and 80 GB system RAM, requested via `--gpus=h100:1 --cpus-per-task=8 --mem=80G`.
    Due to the dynamic scheduler on Alliance, the exact node ID may vary,
    but any node with a full H100 (≈80 GB GPU memory) and similar CPU/RAM
    should be sufficient to reproduce the SFT runs.
